my data <- read.table("Twitter_Data_1",header=TRUE,sep=",")
my_data <- read.table("Twitter_Data_1",header=TRUE,sep=",")
my_data <- read.table("\Downloads|Twitter_Data_1",header=TRUE,sep=",")
my_data <- read.table("\Downloads\Twitter_Data_1",header=TRUE,sep=",")
my_data <- read.table("Downloads\Twitter_Data_1",header=TRUE,sep=",")
setwd("C:\\Users\\GireshMulani\\Downloads")
setwd(Users\\GireshMulani\\Downloads")
setwd("Users\\GireshMulani\\Downloads")
setwd("Users\\Giresh Mulani\\Downloads")
q()
?dbinom
pnorm()
pnorm(q, mean = -0.33, sd = 1, lower.tail = TRUE, log.p = FALSE)
pnorm(-0.33)
Y=0:10
dgeom(Y, 0.5)
plot(dgeom(Y, 0.5))
plot(dgeom(Y, 0.5), x-axis = y, y-axis = probability)
plot(dgeom(Y, 0.5), xlab = y, ylab = probability)
plot(dgeom(Y, 0.5), xlab = Y, ylab = probability)
plot(dgeom(Y, 0.5), xlab = Y, ylab = p)
plot(dgeom(Y, 0.5), xlab = Y, ylab = "probability")
plot(dgeom(Y, 0.5), xlab = "y", ylab = "probability")
plot(dgeom(Y, 1/3), xlab = "y", ylab = "probability")
plot(dgeom(Y, 0.2), xlab = "y", ylab = "probability")
legend(plot(dgeom(Y, 0.2), xlab = "y", ylab = "probability"))
library(glmnet)
library(tree)
library(randomForest)
library(kknn)
setwd("~/Google Drive/Monash/FIT2086/Assignment 3")
source("tree.wrappers.R")
source("wrappers.R")
heart = read.csv("heart.csv")
tree.heart = tree(heart)
summary(tree.heart)
cv.heart = learn.tree.cv(heart.tree, nfolds = 10, m = 20)
cv.heart = learn.tree.cv(tree.heart, nfolds = 10, m = 20)
plot(cv.heart$cv.heart)
summary(cv.heart)
summary(tree.heart)
cv.heart = learn.tree.cv(tree.heart, nfolds = 100, m = 20)
summary(cv.heart)
cv.heart100 = learn.tree.cv(tree.heart, nfolds = 100, m = 50)
cv.heart10 = learn.tree.cv(tree.heart, nfolds = 10, m = 50)
plot(cv.heart10)
plot(cv.heart10$best.tree)
plot(cv.heart100$best.tree)
summary(best.tree)
summary(tree.heart)
text(cv.heart$best.tree, pretty = 12)
text(tree.heart$best.tree, pretty = 12)
text(cv.heart$best.tree, pretty = 12)
text(cv.heart$best.tree, pretty = 15)
text(cv.heart$best.tree, pretty = 1)
text(cv.heart$best.tree, pretty = 12)
summary(tree.heart)
tree.heart
glm(heart)
cv.heart = learn.tree.cv(tree.heart, 10, 1000)
cv.heart
summary(cv.heart)
summary(cv.heart$best.tree)
text(cv.heart$best.tree, pretty = 12)
plot(cv.heart$best.tree, cv.heart$best.tree)
text(cv.heart$best.tree, pretty = 12)
summary(cv.heart)
summary(cv.heart$best.tree)
cv.heart
glm(heart)
glm(data = heart)
glm(heart)
glm.heart = glm(heart)
step(glm.heart, direction = "both", criterion = "BIC")
man glm
man(glm)
help glm
glm(tree.heart)
glm(heart)
glm(cv.heart)
glm(heart)
BIC(glm.heart)
step(glm.heart, direction = "both", criterion = "BIC", k = log(n))
step(glm.heart, direction = "both", criterion = "BIC", k = log(302))
bestglm(heart, IC="BIC")
library(bestglm)
install.packages("best.glm")
install.packages("bestglm")
library(bestglm)
bestglm(heart, IC="BIC")
stepAIC(object=heart, direction="both", k=2179.141)
step(object=heart, direction="both", k=2179.141)
forward.fit.bic = step(glm.heart, k = log(nrow(heart)), trace = 0, direction="both")
install.packages("naivebayes")
library(naivebayes)
glm.heart = glm(HD ~ ., data = heart, family = binomial)
glm.heart
step.fit.bic = step(glm.heart, k = log(nrow(heart)), direction = "both", trace = 0)
step.fit.bic
step.fit.bic = step(glm.heart, k = log(nrow(heart)), direction = "both")
prediction = predict(glm.heart, heart)
summary(prediction)
prediction
fullmod = glm(HD ~ ., + .*., log(AGE) + log(SEX) + log(CP) + log(TRESTBPS) + log(CHOL) + log(FBS) + log(RESTECG) + log(THALACH) + log(EXANG) + log(OLDPEAK) + log(SLOPE) + log(CA) + log(THAL) + log(HD) + I(AGE^2) + I(SEX^2) + I(CP^2) + I(TRESTBPS^2) + I(CHOL^2) + I(FBS^2) + I(RESTECG^2) + I(THALACH^2) + I(EXANG^2) + I(OLDPEAK^2) + I(SLOPE^2) + I(CA^2) + I(THAL^2) + I(HD^2), data = heart, family = binomial)
fullmod = glm(HD ~ ., + .*. + log(AGE) + log(SEX) + log(CP) + log(TRESTBPS) + log(CHOL) + log(FBS) + log(RESTECG) + log(THALACH) + log(EXANG) + log(OLDPEAK) + log(SLOPE) + log(CA) + log(THAL) + log(HD) + I(AGE^2) + I(SEX^2) + I(CP^2) + I(TRESTBPS^2) + I(CHOL^2) + I(FBS^2) + I(RESTECG^2) + I(THALACH^2) + I(EXANG^2) + I(OLDPEAK^2) + I(SLOPE^2) + I(CA^2) + I(THAL^2) + I(HD^2), data = heart, family = binomial)
fullmod = glm(HD ~ ., + log(AGE) + log(SEX) + log(CP) + log(TRESTBPS) + log(CHOL) + log(FBS) + log(RESTECG) + log(THALACH) + log(EXANG) + log(OLDPEAK) + log(SLOPE) + log(CA) + log(THAL) + log(HD) + I(AGE^2) + I(SEX^2) + I(CP^2) + I(TRESTBPS^2) + I(CHOL^2) + I(FBS^2) + I(RESTECG^2) + I(THALACH^2) + I(EXANG^2) + I(OLDPEAK^2) + I(SLOPE^2) + I(CA^2) + I(THAL^2) + I(HD^2), data = heart, family = binomial)
summary(step.fit.bic)
predict.glmnet(glm.heart)
predict(glm.heart)
summary(predict(glm.heart))
summary(predict(glm.heart, type="response"))
clear
rm(list=ls())
q2test = read.csv("q2.test.csv")
q2train = read.csv("q2.train.csv")
source("wrappers.R")
source(tree.wrappers.R)
source("tree.wrappers.R")
kernel = "rectangular"
View(q2test)
View(q2train)
knn = train.kknn(Y ~ ., data = q2.train, kmax = 25, kernel = kernel)
knn = train.kknn(Y ~ ., data = q2train, kmax = 25, kernel = kernel)
knn = train.kknn(y ~ ., data = q2train, kmax = 25, kernel = kernel)
knn
ytest.hat = fitted(kknn(y ~., q2train, q2test, kernel = knn$best.parameters$kernel, k = knn$best.parameters$k))
ytest.hat
plot(ytest.hat)
plot(ytest.hat, xlab = "Independent Variable", ylab = "Dependent Variable")
plot(ytest.hat, xlab = "Independent Variable", ylab = "Dependent Variable", main = "Q2")
ytest.hat = fitted(kknn(y ~., q2train, kernel = knn$best.parameters$kernel, k = knn$best.parameters$k))
kernel = c("rectangular")
knn = train.kknn(y ~ ., data = q2train, kmax = 25, kernel = kernel)
ytest.hat = fitted(kknn(y ~., q2train, q2test, kernel = knn$best.parameters$kernel, k = knn$best.parameters$k))
plot(ytest.hat, xlab = "x", ylab = "f(x)", main = "Q2.1.a.1")
ytest.hat = fitted(kknn(y ~., q2test, kernel = knn$best.parameters$kernel, k = knn$best.parameters$k))
plot(q2test, xlab = "x", ylab = "f(x)", main = "Q2.1.a.1")
plot(knn)
plot(ytest.hat, xlab = "x", ylab = "f(x)", main = "Q2.1.a.1")
lines(stats::lowess(q2test))
lines(stats::lowess(ytest.hat))
plot(ytest.hat, xlab = "x", ylab = "f(x)", main = "Q2.1.a.1")
plot(q2test, xlab = "x", ylab = "f(x)", main = "Q2.1.a.2")
plot(ytest.hat, xlab = "x", ylab = "f(x)", main = "Q2.1.a.1")
lines(q2test, col="red")
plot(ytest.hat, xlab = "x", ylab = "f(x)", main = "Q2.1.a.1")
lines(q2test, q2test, col="red")
plot(q2test, xlab = "x", ylab = "f(x)", main = "Q2.1.a.2")
plot(ytest.hat, xlab = "x", ylab = "f(x)", main = "Q2.1.a.1")
plot(q2train, xlab = "x", ylab = "f(x)", main = "Q2.1.a.1")
lines(q2test, col="red")
plot(q2train, xlab = "x", ylab = "f(x)", main = "Q2.1.a.1", col="blue")
lines(q2test, col="red")
plot(q2train, xlab = "x", ylab = "f(x)", main = "Q2.1.a.1, Q2.1.a.2", col="blue")
lines(q2test, col="red")
knn5 = train.kknn(y ~ ., data = q2train, k = 5, kernel = kernel)
knn5 = train.kknn(y ~ ., data = q2train, ks = 5, kernel = kernel)
knn10 = train.kknn(y ~ ., data = q2train, ks = 10, kernel = kernel)
knn25 = train.kknn(y ~ ., data = q2train, ks = 25, kernel = kernel)
knn1 = train.kknn(y ~ ., data = q2train, ks = 1, kernel = kernel)
plot(knn1)
plot(knn5)
plot(knn10)
plot(knn15)
plot(knn25)
ytest.hat1 = fitted(kknn(y ~., q2train, q2test, kernel = knn1$best.parameters$kernel, k = knn1$best.parameters$k))
ytest.hat5 = fitted(kknn(y ~., q2train, q2test, kernel = knn5$best.parameters$kernel, k = knn5$best.parameters$k))
ytest.hat10 = fitted(kknn(y ~., q2train, q2test, kernel = knn10$best.parameters$kernel, k = knn10$best.parameters$k))
ytest.hat25 = fitted(kknn(y ~., q2train, q2test, kernel = knn25$best.parameters$kernel, k = knn25$best.parameters$k))
plot(ytest.hat1)
plot(ytest.hat1, xlab = "x", ylab = "f(x)", col = "blue")
plot(ytest.hat1, xlab = "x", ylab = "f(x)", col = "blue", legend(legend = "Blue -> k = 1" "Red -> k=5" "Green -> k=10" "Black -> k=25"))
plot(ytest.hat1, xlab = "x", ylab = "f(x)", col = "blue", legend(legend = "Blue -> k = 1"))
lines(ytest.hat5, col="Red")
plot(ytest.hat1, xlab = "x", ylab = "f(x)", col = "blue", legend(legend = "Blue -> k = 1"), type = "l")
plot(ytest.hat1, xlab = "x", ylab = "f(x)", col = "blue", type = "l"
plot(ytest.hat1, xlab = "x", ylab = "f(x)", col = "blue", type = "l")
plot(ytest.hat1, xlab = "x", ylab = "f(x)", col = "blue", type = "l")
lines(ytest.hat5, col="Red")
lines(ytest.hat10, col="Red")
lines(ytest.hat10, col="Green")
lines(ytest.hat25, col="Black")
legend("topleft", title = "Legend", col = "Blue", col = "Green", col = "Red", col = "Black")
legend("topleft", title = "Legend", c("1","5","10","25"))
legend("topleft", title = "Legend", c("Blue -> k=1","Red -> k=5","Green -> k=10","Black -> k=25"))
cv.kknn(y ~ ., data = q2test, kmax=25, kernel = kernel)
cv.kknn(y ~ ., data = q2test, kernel = kernel)
cvkknn = cv.kknn(y ~ ., data = q2test, kernel = kernel)
summary(cvkknn)
cvkknn
cvkknn$best.parameter$k
rf.q2 = randomForest(y ~ ., date = q2train)
rf.q2 = randomForest(y ~ ., data = q2train)
rf.q2
predict(rf.q2, q2test)
sqrt(mean((predict(rf.q2, q2test))))
sqrt(mean((predict(rf.q2, q2test) - q2test$y)^2))
rf.q2 = randomForest(y ~ ., data = q2train, importance = TRUE, ntree = 5000)
sqrt(mean((predict(rf.q2, q2test) - q2test$y)^2))
ytest.hat.predict = fitted(kknn(y ~ ., q2train, q2test))
sqrt(mean((ytest.hat.predict - q2test$y)^2))
lasso.fit = cv.glmnet.f(f, data=q2train, family="binomial",nfolds=20)
formula = my.make.formula("Q2", q2train, use.interactions = T, use.logs = T, use.squares = T, use.cubics = T)
lasso.fit = cv.glmnet.f(formula, data=q2train, family="continuous", nfolds=20)
formula = my.make.formula("y", q2train, use.interactions = T, use.logs = T, use.squares = T, use.cubics = T)
lasso.fit = cv.glmnet.f(formula, data=q2train, family="continuous", nfolds=20)
View(cv.glmnet.f)
lasso.fit = cv.glmnet.f(formula, data=q2train, family="cox", nfolds=20)
lasso.fit = cv.glmnet.f(formula, data=q2train, family="multinomial", nfolds=20)
lasso.fit = cv.glmnet.f(formula, data=q2train, nfolds=20)
plot(lasso.fit)
glmnet.tidy.coef(lasso.fit)
cv.glmnet.f(lasso.fit)
cv.glmnet.f(formula = formula, data = lasso.fit)
read.csv("heart.csv")
heart = read.csv("heart.csv")
tree.heart = tree(heart)
cv.heart = learn.tree.cv(tree.heart,nfolds=10,m=1000)
prune.tree(tree.heart, k = cv.heart$best.k)
summary(cv$best.tree)
summary(cv.heart$best.tree)
prune.tree(tree.heart, k = cv.heart$best_k)
summary(tree.heart)
prune.tree(tree.heart, k = cv.heart$best_k)
prune.tree(tree.heart, k = cv.heart$best.k)
plot(cv.heart$cv.heart)
tree.heart = tree(HD ~ .,heart.csv)
tree.heart = tree(HD ~ .,heart)
cv.heart = learn.tree.cv(tree.heart, 10, 1000)
prune.tree(tree.heart, k = cv.heart$best.k)
summary(cv.heart$best.tree)
sqrt(mean((predict(cv.heart$best.tree, heart) - heart$HD)^2))
predict.glmnet.f(lasso.fit, q2test, type="response")
lasso.pred = predict.glmnet.f(lasso.fit, q2test, type="response")
summary(lasso.pred)
plot(lasso.fit)
min(lasso.fit$cvm)
plot(lasso.fit)
glmnet.tidy.coef(lasso.fit)
min(lasso.fit$cvm)
lasso.fit = glmnet.f(y ~ ., q2train)
lasso.fit = glmnet.f(y ~ ., q2train, lambda = 0)
lines(q2test)
plot(lasso.fit)
lines(q2test, col="green")
cv.heart
summary(cv.heart)
summary(tree.heart)
tree.heart
summary(tree.heart$best.tree)
summary(tree.heart)
tree.heart
plot(tree.heart)
text(cv.heart$best.tree, pretty=12)
text(cv.heart$best.tree, pretty=12)
cv.heart$best.tree
plot(cv.heart$best.tree)
text(cv.heart$best.tree, pretty=12)
glm.heart = glm(HD ~ ., data=heart)
glm.heart = glm(HD ~ ., data = heart, family = binomial)
step.fit.bic = step(glm.heart, k = log(nrow(heart)), direction = "both", trace = 0)
summary(step.fit.bic)
step.fit.bic = step(glm.heart, k = log(nrow(heart)), direction = "both")
predict(step.fit.bic, heart, type="response")
glm(heart)
glm.heart = glm(heart)
summary(glm.heart)
glm.heart = glm(HD ~ ., data=heart)
glm.heart = glm(HD ~ ., data = heart, family = binomial)
glm.heart
summary(glm.heart)
